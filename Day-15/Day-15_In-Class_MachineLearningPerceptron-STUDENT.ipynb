{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; **Put your name here** </p>\n",
    "#### <p style=\"text-align: right;\"> &#9989; Put your group member names here</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 15 In-Class Assignment: Introduction to Machine Learning \n",
    "\n",
    "\n",
    "<img src=\"https://goo.gl/FYHM5q\" width=600pc >\n",
    "<p style=\"text-align: right;\">Image from: https://goo.gl/ypY9G2</p>\n",
    "\n",
    "1. **Scientific motivation** \n",
    "    - Classifying data (iris types) \n",
    "2. **Modeling tools** \n",
    "    - Machine Learning (Perceptron)\n",
    "3. **Programming concepts** \n",
    "    - Creating Classes and re-usable code\n",
    "    - Pulling in data from outside sources \n",
    "    - Using external libraries \n",
    "    \n",
    "\n",
    "### Agenda for today's class\n",
    "\n",
    "</p>\n",
    "\n",
    "1. Review of pre-class assignment\n",
    "1. Problem Statement\n",
    "1. Basics of the perceptron model\n",
    "1. Loading and inspecting the data\n",
    "1. Building the perceptron model\n",
    "1. Plotting the decision boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 1. Review of pre-class assignment\n",
    "\n",
    "Were there any specific questions that came up in the pre-class assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Problem Statement\n",
    "    \n",
    "\n",
    "We want to build a model that can accurately classifying two types of flowers based off of measurements we have collected. Building this model will allow us to understand how basic machine learning models learn and what exactly is happening 'under the hood'. It will provide a slightly more intuitive view of machine learning and make it seem as less of a black box and more of a tool that we understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Basics of the Perceptron Model\n",
    "\n",
    "The perceptron that we will be working with in this assignment is a what is known as a basic binary classifier. It takes in a set of _training data_ that is linearly separable and then computes a *set of weights* and a *bias term* to apply to _input data_ as to properly classify it. This perceptron only works for data that contains two classes and is linearly separable. For data that does not have these properties, the classifier cannot properly learn the weights and bias term. There are more extensive perceptron tools out there that can work with more sophisticated datasets, such as the [Multi-Layer Perceptron model in Scikit-Learn](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
    "\n",
    "Since this perceptron is based on linearly separable data, we can think of the model as trying to learn the slope of a line, $y = m~x + b$. This may look very familiar to you as the slope-intercept form for a linear function. However in machine learning, we usually define $X$ to be an input *vector*, which is a $N$ by $1$ matrix, where $N$ is the number of measurements or \"features\" for a given sample. Now that we have $X$ defined, we can create a similar matrix of weights, $W$, of length $1$ by $N$, and re-write our equation to be:\n",
    "\n",
    "$$ Y = W \\cdot X + B$$\n",
    "\n",
    "where $Y$ represents the resulting classification, consisting of either -1 or 1, depending on the output of $W \\cdot X$, the dot product of $W$ and $X$, and $B$ represents the bias term. For those of you unfamiliar with linear algebra, a dot product is an operation where we add up the pairwise multiplication of each of the elements of two vectors. So, $W \\cdot X = w_0 x_0 + w_1 x_1 + \\dots + w_N x_N$. This will give us a single number, which we can then add our bias to (add some offset, like the y-intercept for a line). More explicitly, we can look at this in a matrix format:\n",
    "\n",
    "$$ Y = \\begin{bmatrix} w_1 & w_2 & \\dots & w_n \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} + B $$\n",
    "\n",
    "But, **how do we go about learning the weights of the model?**\n",
    "\n",
    "We learn the model weights by attempting to predict the class of our input data using an initial guess for the weights, and then update our weight values based on our prediction. In essence, we need some amount of labeled data to train the model. Our weights are going to initially be defined by some guess. For the purposes of this assignment, we will start our model out by _randomly_ assigning weights to each element of $W$. Now, as we saw in the pre-class, random weights don't do much for making good predictions. So, we will need to use our training data to tune our weights. To start, we can define a \"step size\" for how much we change our weights between subsequent guesses in the following way:\n",
    "\n",
    "`step_size = eta * (target class - predicted class)` \n",
    "\n",
    "You can think of this as how coarse of an adjustment we make to our weights at any given iteration. If we make $\\eta$ (eta) very large, then our resulting weights will be influence much more by the differences between the predicted class and true class. If we make $\\eta$ small, we will only make small adjustments each iteration. Each approach has its strengths and weaknesses, so you need to consider this when building your model. Then, using our \"step_size\", we then update all of our weights by multiplying our *step size times the corresponding feature values*:\n",
    "\n",
    "$$w_{1,new} = w_{1,old} + (\\mathrm{step~size} \\times x_{1})$$\n",
    "$$w_{2,new} = w_{2,old} + (\\mathrm{step~size} \\times x_{2})$$\n",
    "$$ \\vdots $$\n",
    "$$w_{N,new} = w_{N,old} + (\\mathrm{step~size} \\times x_{N})$$\n",
    "\n",
    "We also have to update our bias term, but just use the step size for this update: $B_{new} = B_{old} + \\mathrm{step~size}$. In this way, the bias can just be thought of as all of the differences between the true labels and predicted labels.\n",
    "\n",
    "In this model, we use $\\eta$ to represent our \"learning_rate\", which takes on a value between 0 and 1.\n",
    "\n",
    "The step size should always be a positive or negative decimal value depending on $\\eta$. For example, if we set the learning rate, $\\eta$, to be .1 and our target is -1 and we predict 1 then the we get the following equation.\n",
    "\n",
    "`step_size = .1 * (-1-1) = -.2`\n",
    "   \n",
    "Alternatively, if our target is 1 and we predict it as -1 then we will get the following.\n",
    "\n",
    "`step_size = .1 * (1 - -1) = .2`\n",
    "\n",
    "This process occurs iteratively. So, for a set number of iterations we calculate a step size and adjust the weights accordingly.\n",
    "\n",
    "**But, how do we handle the \"learning\" process when we have multiple samples?** Our dataset consists of many data points, each with its own features, that we want to try and utilize to build a more robust classification model.\n",
    "\n",
    "In this case, we need to update the weights based on _all_ of the sample features. If our training data consists of $M$ points, then our original equation above becomes:\n",
    "\n",
    "$$w_{1,new} = w_{1,old} + \\sum_{i=0}^{M} (\\mathrm{step~size} \\times x_{i,1})$$\n",
    "$$w_{2,new} = w_{2,old} + \\sum_{i=0}^{M} (\\mathrm{step~size} \\times x_{i,2})$$\n",
    "$$ \\vdots $$\n",
    "$$w_{N,new} = w_{N,old} + \\sum_{i=0}^{M} (\\mathrm{step~size} \\times x_{i,N})$$\n",
    "\n",
    "In the sum, $x_{i,1}$ represents the 1st feature of the $i$th data point. In this way, we update the weight for feature 1 by looking at the first feature for _all_ of our samples. \n",
    "\n",
    "We also need to make sure we update the bias value in a similar way:\n",
    "\n",
    "$$B_{new} = B_{old} + \\sum_{i=0}^{M} (\\mathrm{step~size})$$\n",
    "\n",
    "**When you get to the point of trying to code all of this up, it will be important that you talk this through with your group and hash out a plan for how to make everything work!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading and inspecting the data\n",
    "\n",
    "Before we build a machine learning model, we need data to base it off of. The data set we are going to use has been provided for you in the directory for this assignment, `iris.csv`. This dataset contains measurements for the properties of two different iris varieties.\n",
    "\n",
    "**Load the data into python and visualize (with a plot) to get a sense for what it looks like. Use different colors to represent the two different iris classifications.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do This: Load in the iris.csv file and plot the data based on the iris classifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: Is the data linearly separable? How many data points do we have? How many of each class?\n",
    "\n",
    "**Put your answers in the cell below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Building the perceptron model\n",
    "\n",
    "Now that we have some data to work with, we want to start building a model, Part 3 outlines how to use the perceptron model to fit the data and properly update weights. Your job is to create a Python `perceptron` class that matches the following specifications:\n",
    "\n",
    "> * Define the perceptron class with an `__init__` method\n",
    "    - The class should be initialized with the following attributes:\n",
    "        - user defined input value for `eta`, the learning rate for the perceptron.\n",
    "        - a number of iterations to be used by the model, `n_iter`. This should also be an input parameter.\n",
    "        - an initial value for the bias (you can choose whether or not the user can set this values or if you want a standard default).\n",
    "* Create two methods for the perceptron class, a `fit` method that does the learning and a `predict` method that outputs the predicted class\n",
    "    - The `fit` method should:\n",
    "        - define an array of weights the same length as the input vector. You can choose how to initialize the weight values.\n",
    "        - go through a set number of iterations (based on `n_iter`) where it makes predictions and updates the weights vector accordingly for each subsequent round of predictions. Each iteration, you should be calculating $w_{new}$ and using the weights to predict the class for the sample. Refer to the pseudocode to help you structure this. \n",
    "    - The `predict` method should:\n",
    "        - take in a feature vector and return the predicted class based on the current weights\n",
    "        - *hint*: The prediction is just a dot product of the weights and the features plus the bias term. The resulting ouput should be in the range (-1,1), depending on the value of the dot product. If the prediction is less than 0 it should return -1, otherwise it should return 1.\n",
    "        - *hint*: You may find it useful to define this method to predict the class for the `fit` method as you will need to predict the class for each sample at every training iteration in order to calculate step_size. The method only needs a feature vector as an input!\n",
    "        \n",
    "This is a challenging class to code from scratch. **Working as a group is going to be extremely beneficial in getting to a solution.** There's a reason why you're at a table with several other people working on the same problem!\n",
    "\n",
    "### Recapping the mathematics:\n",
    "(to reduce the need to scroll)\n",
    "\n",
    "`step_size = eta * (target class - predicted class)` \n",
    "\n",
    "$$w_{1,new} = w_{1,old} + \\sum_{i=0}^{M} (\\mathrm{step~size} \\times x_{i,1})$$\n",
    "$$w_{2,new} = w_{2,old} + \\sum_{i=0}^{M} (\\mathrm{step~size} \\times x_{i,2})$$\n",
    "$$ \\vdots $$\n",
    "$$w_{N,new} = w_{N,old} + \\sum_{i=0}^{M} (\\mathrm{step~size} \\times x_{i,N})$$\n",
    "\n",
    "$$B_{new} = B_{old} + \\sum_{i=0}^{M} (\\mathrm{step~size})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this: Write a Perceptron class to classifying our data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the new perceptron class\n",
    "\n",
    "Now that we have a classifier built, we need to test it on data, but first we need to make sure our data is in the right format so we can properly train a classifier on it. This means, if you haven't already, that you need to make sure that you classes are either -1 or 1. You'll also need to make sure the features vectors can be fed into your perceptron class correctly.\n",
    "\n",
    "Also, remember, for a good model, we want the training data to have an even sample of both classes so it knows what to look for and doesn't end up biased towards a particular classifications.\n",
    "\n",
    "As a rule of thumb, you should use ~75% of your sample data as your training data and reserve the remaining ~25% as testing data. Make sure that the corresponding feature vector for each class label is used. If you do some kind of shuffling of the data, make sure that you don't lose track of which features go with which label. Make sure that all of your data is numeric, too. The model will not work if the true label is \"Iris-setosa\" or \"Iris-versicolor\". It needs numerical values.\n",
    "\n",
    "**Using your new perceptron class, train the model using your training and then test the results on your testing data.** You may want to come up with a method for computing how many predictions are right versus wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure you data is in the right format to be fed into your perceptron class and split your data into a training set and a testing set\n",
    "\n",
    "\n",
    "# Then, train your model using your `fit` method.\n",
    "\n",
    "\n",
    "# Finally, test your trained model on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Plotting the decision boundary \n",
    "\n",
    "Finally, to better understand our classifier, it might help to plot what is known as the \"decision boundary\". The decision boundary is the line that seperates the classes in the classifier. The line is defined by the weights and the bias term that we calculated for our model.\n",
    "\n",
    "The slope of the decision boundary is defined as:\n",
    "\n",
    "$$ m = -\\frac{w_1}{w_2} $$\n",
    "\n",
    "And the $y$-intercept, $b$, is defined as:\n",
    "\n",
    "$$ b = -\\frac{B}{w_2} $$\n",
    "\n",
    "You should be able to generate a set of evenly spaced $x$-axis values and then used the equation for a line ($y = mx + b$) to compute the decision boundary for making a plot of the line. You should get something that looks like this:\n",
    "\n",
    "<img src=\"https://i.imgur.com/UPX8XDy.png\">\n",
    "\n",
    "**As a note:** If you normalized your feature data before you trained your perceptron, then you will need to plot the _scaled_ feature data with the boundary line. Otherwise, the units will not agree and the plot will not come out properly. Also, remember that when normalizing your feature data, you have to divide _each_ column by the maximum of that _that_ column, not just the maximum of the entire array. **if you did not normalize your feature data before training your perceptron class,** that's perfectly fine. The results are still correct and the plot will come out properly. You do NOT need to normalize the data at this point if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the decision boundary and make a plot of it, along with the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Assignment Wrap-up\n",
    "\n",
    "Fill out the following Google Form before submitting your assignment to D2L!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\n",
    "\"\"\"\n",
    "<iframe \n",
    "\tsrc=\"https://goo.gl/forms/noAwoJd9pEj7A1su1\" \n",
    "\twidth=\"800px\" \n",
    "\theight=\"600px\" \n",
    "\tframeborder=\"0\" \n",
    "\tmarginheight=\"0\" \n",
    "\tmarginwidth=\"0\">\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Congratulations, we're done!\n",
    "\n",
    "Now, you just need to submit this assignment by uploading it to the course <a href=\"https://d2l.msu.edu/\">Desire2Learn</a> web page for today's submission folder (Don't forget to add your names in the first cell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2018,  Michigan State University Board of Trustees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
