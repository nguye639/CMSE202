{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; **Put your name here** </p>\n",
    "#### <p style=\"text-align: right;\"> &#9989; Put your group member names here</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 18 In-Class Assignment: Principal Component Analysis (PCA)\n",
    "\n",
    "<img src=\"http://lazyprogrammer.me/wp-content/uploads/2015/11/PCA.jpg\" width=\"400\"><a href=\"http://lazyprogrammer.me/tutorial-principal-components-analysis-pca//\"><p style=\"text-align: right;\">\n",
    "Image from http://lazyprogrammer.me/\n",
    "</p></a>\n",
    "\n",
    "PCA Explained Visually: [http://setosa.io/ev/principal-component-analysis/](http://setosa.io/ev/principal-component-analysis/)\n",
    "\n",
    "\n",
    "1. **Scientific motivation** \n",
    "    - Biometrics (Pattern Recognition)\n",
    "2. **Modeling tools** \n",
    "    - Principal Component Analysis (PCA) for feature selection\n",
    "3. **Programming concepts** \n",
    "    - Code Review\n",
    "4. **Python Programming Concepts** \n",
    "    - Saving objects using the pickle library\n",
    "    \n",
    "\n",
    "\n",
    "# Problem Description\n",
    "In Machine Learning, a good \"rule of thumb\" is to have more than 10 times the number of training samples as dimensions in your feature vector.  So if your feature vector has 4 features, then you should have 40 samples for training (this doesn't count the samples for testing).\n",
    "\n",
    "This is also called the \"curse of dimensionality\" and can sometimes be counter intuitive.  Basically we want more dimensions to ensure that we have captured the patterns we want to learn in the model. However, too many dimensions requires more training data. This can be a big problem when you have high dimensional data. \n",
    "\n",
    "For example, the face images provided by sci-kit learn, which we visited briefly at the end of the last class period and are using today in class, have 50 rows and 37 columns; this results in a vector length of 1850.  By the Machine Learning rule of thumb, we should have 18,500 examples in our training set (we only have at most 1560 examples and that does not include the ones we have set aside for testing. Many of you saw what happened when we ignored this rule.\n",
    "\n",
    "In this assignment we are going to use Principal Component Analysis (PCA) to to reduce the size of our feature vector and only select features which are the \"most\" descriptive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda for today's class\n",
    "\n",
    "</p>\n",
    "\n",
    "1. Review of Face Recognition from last time\n",
    "1. Pre-class assignment review\n",
    "1. Feature Selection using Principal Component Analysis\n",
    "1. Picking the best Feature Vectors\n",
    "1. Eigenfaces (just for fun -- and knowledge!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Review of Face Recognition from last time\n",
    "\n",
    "&#9989; For each of the following steps; run the code and discuss as a group what the code is doing.  As a class we will continue the discussion. \n",
    "\n",
    "First, lets download and view the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the data from scikit-learn\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_lfw_people, load_digits\n",
    "\n",
    "sk_data = fetch_lfw_people(min_faces_per_person=50, resize=0.4)\n",
    "\n",
    "feature_vectors = sk_data.data\n",
    "class_labels = sk_data.target\n",
    "categories = sk_data.target_names\n",
    "n_samples, n_features = feature_vectors.shape\n",
    "N, h, w = sk_data.images.shape\n",
    "n_classes = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "def browse_images(images, labels, categories):\n",
    "    n = len(images)\n",
    "    def view_image(i):\n",
    "        plt.imshow(images[i], cmap=plt.cm.gray, interpolation='nearest')\n",
    "        plt.title('%s' % categories[labels[i]])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    interact(view_image, i=(0,n-1))\n",
    "browse_images(sk_data.images, sk_data.target, sk_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Splitting the dataset into training and testing sets\n",
    "\n",
    "Let's split the data into a training and testing set using the ```train_test_split``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Note, if you have an older version of scikit-learn,\n",
    "# you might need to replace the above line with the one below\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(feature_vectors, \n",
    "                                                                          class_labels, \n",
    "                                                                          random_state=1,\n",
    "                                                                          test_size=0.25)\n",
    "print(len(train_vectors))\n",
    "print(len(test_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train an SVM Classifier based on the training dataset.\n",
    "\n",
    "As we learned in the last class.  This training step can take a while (more than 5 minutes, depending on your computer).  To save some time, the instructors modified the code from last time to save the results of the training data to a file and saved this file in the class git repository.  You can used the saved data by setting ```rerun_training``` to `False` or, if you want, you can redo the training step by setting ```rerun_training``` to `True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rerun_training = False\n",
    "filename = 'full_face_model.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a SVM classification model  NOTE This can take ~ 5 minutes or more!!!!\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# If you have an old version of sklearn that you have upgraded yet,\n",
    "# you will need to change the above import to the one below\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "#make some temporary variables so you can change this easily\n",
    "tmp_vectors = train_vectors\n",
    "tmp_labels = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "if rerun_training:\n",
    "    \n",
    "    print(\"Fitting the classifier to the training set\")\n",
    "    param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "                  'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "    clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "    clf = clf.fit(tmp_vectors, tmp_labels)\n",
    "    print(\"Best estimator found by grid search:\")\n",
    "    print(clf.best_estimator_)\n",
    "\n",
    "    #save the model to a file\n",
    "    pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "else:\n",
    "    #read the model from a file\n",
    "    print(\"reading pickle file.\")\n",
    "    clf = pickle.load(open(filename, 'rb'))\n",
    "    print(\"Best estimator found by grid search:\")\n",
    "    print(clf.best_estimator_)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"Runtime\",end - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What library and functions are used to save and load a python object in this example?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=8 color=\"#009600\">&#9998;</font> Do This - Erase the contents of this cell and replace it with your answer to the above question!  (double-click on this text to edit this cell, and hit shift+enter to save the text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Show the results of the classification on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "#make some temporary variablse so you can change this easily\n",
    "predict_vectors = test_vectors\n",
    "true_labels = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Predicting people's names on the test set\")\n",
    "pred_labels = clf.predict(predict_vectors)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "print(confusion_matrix(true_labels, pred_labels, labels=range(n_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_gallery(images, true_titles, pred_titles, h, w, n_row=5, n_col=5):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title('Pred='+str(categories[pred_titles[i]]), size=9)\n",
    "        plt.xlabel('Actual='+str(categories[true_titles[i]]), size=9)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "plot_gallery(test_vectors, test_labels, pred_labels, h,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Review of pre-class assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 3. Feature Selection using Principal Component Analysis\n",
    "\n",
    "Now we are going to go through the entire program above and use Principal Component Analysis (PCA) to select a much smaller set of better features.   The following code will reduce the original picture feature vector to a feature vector with 4 (n_components).  This is often called unsupervised feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 4 # This is much less than the original n_features\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\" % (n_components, train_vectors.shape[0]))\n",
    "\n",
    "#Set up the pca object with the number of compoents we want to find\n",
    "pca = PCA(n_components=n_components, whiten=True)\n",
    "\n",
    "#Fit the training data to the pca model.\n",
    "_ = pca.fit(train_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a ```pca``` object that has our data fit to it.  We can use this object to transform the original training vectors consisting of the entire image into new training vectors using the ```transform``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_train_vectors = pca.transform(train_vectors)\n",
    "pca_test_vectors = pca.transform(test_vectors)\n",
    "\n",
    "print(\"Training set changed from a size of: \", train_vectors.shape, ' to: ', pca_train_vectors.shape)\n",
    "print(\"Testing set changed from a size of: \", test_vectors.shape, ' to: ', pca_test_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **DO THIS:** We have now reduced the size of our feature vectors.  Modify the code in **Section 1** to substitute these two new reduced size vectors for the full size vectors.  This modification should replace both the training vectors and the testing vectors. Make sure you also consider the following:\n",
    "\n",
    "* Recalculate the clf model and do not load it from memory -- **this is important!**\n",
    "* You probably want to **change the filename so you do not overwrite the instructors file**\n",
    "* Make sure your changes ensure that **the tmp_vectors and predict_vectors reference the new smaller vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:  What precision can the SVM algorithm achieve with only the first 4 feature vectors?  And how long did this take to run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=8 color=\"#009600\">&#9998;</font> Do This - Erase the contents of this cell and replace it with your answer to the above question!  (double-click on this text to edit this cell, and hit shift+enter to save the text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Picking the \"best\" Feature Vectors\n",
    "If you remember from your pre-class assignment the PCA algorithm finds a transform of the data such that the first component contains the \"most\" information the second component contains the \"second most\" important information.  How much information each component contains is actually included in the ```pca``` object and can be expressed as a ratio from 0 (no information) to 1 (all information).  Lets plot these values below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lets plot the variance of the eigen values\n",
    "plt.plot(pca.explained_variance_ratio_, marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at this is we can sum up the total ratios and see how much our new set of features represents the \"variance\" in the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_variance = np.sum(pca.explained_variance_ratio_)*100\n",
    "print(\"These %d eigenvectors account for a total of %d percent of the total variance in the original dataset\"\n",
    "      % (n_components, total_variance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How many components would we need to represent 90% of the variance in our original data? (Hint: modify **Section 3** and change n_components to find a total_variance > 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=8 color=\"#009600\">&#9998;</font> Do This - Erase the contents of this cell and replace it with your answer to the above question!  (double-click on this text to edit this cell, and hit shift+enter to save the text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **DO THIS:** generate a plot that generates a bar graph and compares the precision achieved of the following:\n",
    "* original - The original image as a complete feature vector.\n",
    "* 4 - The four \"best\" PCA components\n",
    "* 90% - The components representing the top 90% of the PCA variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 5. Eigenfaces (just for fun and knowledge)\n",
    "\n",
    "An eigenvector is a transform from the original image space into a new space along a single axis.  You can actually think of these vectors as a weighted sum of the components of an original image. So the length of the eigenvectors are the same size as the original data. Let's plot the gallery of the most significant eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eigenfaces = pca.components_.reshape((n_components, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=5):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.7 * n_col, 2.3 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w, n_row=1, n_col=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors are often called eigenfaces.  Each image gives you information about where the greatest amount of variation occurs in the images. Some of the eigenfaces tell you more about the lighting in the image or about the orientation of the face and less about the actual facial features, but others highlight where the most variation is from face to face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Assignment Wrap-up\n",
    "\n",
    "Fill out the following Google Form before submitting your assignment to D2L!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\n",
    "\"\"\"\n",
    "<iframe \n",
    "\tsrc=\"https://goo.gl/forms/noAwoJd9pEj7A1su1\" \n",
    "\twidth=\"800px\" \n",
    "\theight=\"600px\" \n",
    "\tframeborder=\"0\" \n",
    "\tmarginheight=\"0\" \n",
    "\tmarginwidth=\"0\">\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Congratulations, we're done!\n",
    "\n",
    "Now, you just need to submit this assignment by uploading it to the course <a href=\"https://d2l.msu.edu/\">Desire2Learn</a> web page for today's submission folder (Don't forget to add your names in the first cell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2018,  Michigan State University Board of Trustees"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
