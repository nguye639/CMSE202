{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; Put your name here</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #5: Practice with Machine Learning Classification\n",
    "\n",
    "<img src=https://cdn-images-1.medium.com/max/2000/1*e8rrnVN-zhTXRxUUG-jTjg.png width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll work on finishing your perceptron binary classifier or find an example online that you can copy (with proper citation!), modify as necessary, and then run on the iris data file from class. If you already managed to get the perceptron model to work in class, you'll be asked to find a version online and compare it to your solution. The details of all of this are include in Part 1 in the text below.\n",
    "\n",
    "After that, in Parts 2 and 3, you'll work on building an SVM from scratch using a python package that does the optimatization step for you and apply it to the iris data.\n",
    "\n",
    "**You are encouraged to read through the entire assignment before you start to make sure you understand what you're expected to do.**\n",
    "\n",
    "### Goals for this assignment:\n",
    "\n",
    "By the end of this assignment, you should be able to:\n",
    "\n",
    "* Use the perceptron model to classify linearly separable data.\n",
    "* Build a support vector machine (SVM) to classify linearly separable data\n",
    "\n",
    "### Assignment instructions\n",
    "\n",
    "**This assignment is due at 11:59pm on Friday, April 5.** This is an individual assignment, but you should feel free to discuss issues you run into in the CMSE 202 \"help\" channel on Slack. When you're done, it should be uploaded into the \"Homework Assignments\" dropbox folder for Homework #5. Further submission instructions can be found at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Using the perceptron model to classify data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your Day 15 in-class assignment you worked toward building the perceptron classifier from scratch. For this part of the homework assignment **you need to accomplish one of the following options**:\n",
    "\n",
    "1. If you did not finish writing and testing your perceptron class and would like to do so, you should do that here. The details of the perceptron model from the in-class assignment are reproduced below. You should make sure that your classifier works with the `iris.csv` file from class (which is also included in the homework directory of the assignments repository). You should also make the plot that was displayed at the end of the in-class assignment that shows the decision boundary along with the data.\n",
    "\n",
    "2. If you did not finish writing and testing your perceptron class, but are not confident that you were headed down the path with designing your method, you are encourage to find an example on the internet, reproduce it here, cite your source, and run the classifier on the `iris.csv` file from class. Once you get the code working, you should make sure you produce a plot that displays the data and a line for the decision boundary (as described in the in-class assignment).\n",
    "\n",
    "3. If you *did* get your perceptron model working, reproduce it here and confirm that it produces the expected decision boundary by producing a plot. In addition to this, find some example Python code for the perceptron model online, provide a link to the example, review the approach used in the example, and highlight how the approach differed from your approach. Is there anything from the example that you think is better than your solution? Anything that you think is worse? Include this discussion in your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which of the above three options are you completing for this assignment?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your chosen option here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details for building and training the model\n",
    "\n",
    "Remember, when you train the model, you should only use ~75% of the available data as training data and reserve the remaining 25% to performing testing. Once you have a working model, you should **include a test to ensure that the model produces the right classifications on some for you testing data**.\n",
    "\n",
    "You should use the `iris.csv` file contained within the homework directory as your training and testing data.\n",
    "\n",
    "Once everything is working, make sure you plot the data and the decision boundary for your model. \n",
    "\n",
    "**The details for the perceptron model, the suggestions for how to build a class from scratch, and the details for plotting the decision boundary are all reproduced from the in-class at the end of this notebook. Use these as a reference when creating your final solution**.\n",
    "\n",
    "If you decide to use an example from the internet, **make sure to include a proper citation or link to the source!** If you do not, you will not receive full points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Building a support vector machine  from scratch\n",
    "\n",
    "In your Day-17 in-class assignment you called the built-in function in the scikit-learn package to implement the support vector machine (SVM). The goal of this part is to manually build a linear SVM. For simplificy, we work with only two features $(x_1,x_2)$.\n",
    "\n",
    "\n",
    "Given a training dataset $((x^{(1)}_1,x^{(1)}_2),y_1), \\dots, ((x^{(n)}_1,x^{(n)}_2),y_n)$ where $y_i$ are either 1 or -1, indicating the class to which the point $(x^{(i)}_1,x^{(i)}_2)$ belongs, $i=1,\\dots,n$. The idea of the support vector machine is to find a hyperplane that separates the datasets and in the meanwhile maximize the distance between the hyperplane and the nearest point from either group. A good reference is the Wikipedia page (see the Section \"Linear SVM\"): https://en.wikipedia.org/wiki/Support-vector_machine \n",
    "\n",
    "<img src=\"https://i.imgur.com/finNteQ.jpg\" width=400px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a toy problem to show how an SVM is built. Consider four points on the $x_1x_2$-plane: (1,1), (-1,1), (-1,-1), (1,-1). It is recommended that you draw a Cartesian coordinate system and mark these four points. We classify the points into two classes: one is (1,1), (-1,1), denoted by +1; the other is (-1,-1), (1,-1), denoted by -1.\n",
    "\n",
    "It is clear that these two classes can be easily separated by lines. For instance, any line through the origin with slope between -1 and 1 does the job. However, SVM is designed not just to separate the classes, but also maximize the distance between the line and the nearest point from either class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 2.1*** Intuitively, which line separates the two classes and maximizes the distance between the line and the nearest point from either class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually identify the maximum-margin line using knowledge from analytic geometry. Recall that the equation of a line on the $x_1 x_2$-plane can be written as \n",
    "\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + w_0 = 0\n",
    "$$\n",
    "\n",
    "where $- \\frac{w_0}{w_2}$ is the intercept. The following explanation is taken from Wikipedia with slight change of some notations. In our case, hyperplanes are just lines.\n",
    "\n",
    "\n",
    "\"If the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the \"margin\", and the maximum-margin hyperplane is the hyperplane that lies halfway between them. With a normalized or standardized dataset, these hyperplanes can be described by the equations\n",
    "$w_1 x_1 + w_2 x_2 + w_0 = 1$ (anything on or above this boundary is of one class, with label 1)\n",
    "and\n",
    "$w_1 x_1 + w_2 x_2 + w_0 = -1$ (anything on or below this boundary is of the other class, with label âˆ’1).\n",
    "Geometrically, the distance between these two hyperplanes is $\\frac{2}{\\sqrt{w_1^2+w_2^2}}$, as can be computed using the distance from a point to a plane equation; so to maximize the distance between the planes we want to minimize the denominator $\\sqrt{w_1^2+w_2^2}$, or equivalently to minimize its square $w_1^2+w_2^2$. We also have to prevent data points from falling into the margin, we add the following constraint: for each $i$ either\n",
    "\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + w_0 \\geq 1, \\quad\\quad\\quad \\text{ if } y_i = 1;\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + w_0 \\leq -1, \\quad\\quad\\quad \\text{ if } y_i = -1.\n",
    "$$\n",
    "\n",
    "These constraints state that each data point must lie on the correct side of the margin. Notice these two conditions can be combined into the following one:\n",
    "\n",
    "$$\n",
    "y_i (w_1 x_1 + w_2 x_2 + w_0) \\geq 1, \\quad\\quad \\text{ for } i = 1,\\dots,n.\"\n",
    "$$\n",
    "\n",
    "\n",
    "**Take-home message: constructing a support vector machine is reduced to solving the following optimization problem:**\n",
    "\n",
    "**Minimize $w_1^2+w_2^2$ subject to $y_i (w_1 x_1 + w_2 x_2 + w_0) \\geq 1$ for $i = 1,\\dots,n$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us find out these inequality constraints using our datasets $((1,1),1)$, $((-1,1),1)$, $((-1,-1),-1)$, $((1,-1),-1)$. For the first point, $(x_1,x_2)=(1,1)$, $y_1=1$, hence we require\n",
    "\n",
    "\\begin{equation}\n",
    "w_1 + w_2 + w_0 \\geq 1. \n",
    "\\end{equation}\n",
    "\n",
    "For the second point, $(x_1,x_2)=(-1,1)$, $y_2=1$. This leads to\n",
    "\n",
    "\\begin{equation} \\label{ineq2}\n",
    "-w_1 + w_2 + w_0 \\geq 1.\n",
    "\\end{equation}\n",
    "\n",
    "For the third point, $(x_1,x_2)=(-1,-1)$, $y_3=-1$. This gives\n",
    "\n",
    "\\begin{equation} \\label{ineq3}\n",
    "-(-w_1 - w_2 + w_0) \\geq 1.\n",
    "\\end{equation}\n",
    "\n",
    "For the fourth point, $(x_1,x_2)=(1,-1)$, $y_4=-1$, then\n",
    "\n",
    "\\begin{equation} \\label{ineq4}\n",
    "-(w_1 - w_2 + w_0) \\geq 1.\n",
    "\\end{equation}\n",
    "\n",
    "These are the four constraints for the optimization problem. However, the solver we are going to use accepts only inequalities with \"$\\leq$\" (less than or equal to) rather than \"$\\geq$\" (bigger than or equal to). The trick to turn \"$\\geq$\" into \"$\\leq$\" is to multiply each inequality by -1. After the multiplication, the above four inequalities become\n",
    "\n",
    "$$\n",
    "-w_1 - w_2 - w_0 \\leq -1\n",
    "$$\n",
    "\n",
    "$$\n",
    " w_1 - w_2 - w_0 \\leq -1\n",
    "$$\n",
    "\n",
    "$$\n",
    "-w_1 - w_2 + w_0 \\leq -1\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_1 - w_2 + w_0 \\leq -1.\n",
    "$$\n",
    "\n",
    "We will solve this optimization problem using the python package \"cvxopt\". The documentation for the package is located here: [https://cvxopt.org/index.html](https://cvxopt.org/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 2.2*** Install the package \"cvxopt\" using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your command here (either one that you run through this notebook or the one that you ran on the command line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cvxopt package has a built-in function \"cvxopt_solve_qp(P,q,G,h)\" which can solve the optimization problem\n",
    "$$\n",
    "\\text{Minimize:} \\quad\\quad \\frac{1}{2} w^T P w + q^T w\n",
    "$$\n",
    "$$\n",
    "\\text{Subject to:} \\;\\;\\;\\quad \\quad\\quad G w \\leq h.\n",
    "$$\n",
    "Here $P,G$ are matrices, $w,q,h$ are vectors, \"$T$\" means transpose.\n",
    "\n",
    "\n",
    "\n",
    "To write our toy problem in this form, take \n",
    "$$\n",
    "w = \\left[\n",
    "\\begin{array}{c}\n",
    "w_1  \\\\\n",
    "w_2  \\\\\n",
    "w_0\n",
    "\\end{array}\n",
    "\\right], \\quad \n",
    "P = \\left[\n",
    "\\begin{array}{ccc}\n",
    "2 & 0 & 0 \\\\\n",
    "0 & 2 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{array}\n",
    "\\right], \\quad \n",
    "q = \\left[\n",
    "\\begin{array}{c}\n",
    "0  \\\\\n",
    "0  \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right], \\quad \n",
    "G = \\left[\n",
    "\\begin{array}{ccc}\n",
    "-1 & -1 & -1 \\\\\n",
    "1 & -1 & -1 \\\\\n",
    "-1 & -1 & 1 \\\\\n",
    "1 & -1 & 1\n",
    "\\end{array}\n",
    "\\right], \\quad \n",
    "h = \\left[\n",
    "\\begin{array}{c}\n",
    "-1  \\\\\n",
    "-1  \\\\\n",
    "-1  \\\\\n",
    "-1\n",
    "\\end{array}\n",
    "\\right], \\quad \n",
    "$$\n",
    "\n",
    "**Remark: it is important to observe how $G$ and $h$ are formed: first express the four inequalities in terms of \"$\\leq$\" (\"$\\geq$\" must be turned into \"$\\leq$\" using the trick), then $G$ consists of the coefficients of $w_1, w_2, w_0$ in these inequalities, $h$ consists of all -1's.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 2.3*** Following [this guide](https://courses.csail.mit.edu/6.867/wiki/images/a/a7/Qp-cvxopt.pdf) to apply the built-in function \"cvxopt_solve_qp(P,q,G,h)\" (which is accessed through the `solver.qp()` class method). Identify the line given by the outputs. Does this line agree with your guess in Question 2.1?\n",
    "\n",
    "**Caution:** the package \"cvxopt\" has its own data structure to represent 2D arrays called \"matrix\". It is strongly recommended you first construct 2D arrays using NumPy and then convert them into \"matrices\". If you want to define \"matrices\" directly, *be careful that \"matrices\" are specified column by column, not row by row as for NumPy 2D arrays!* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Testing the support vector machine on the iris data\n",
    "\n",
    "In your Day-16 in-class assignment you have classified the iris dataset using a Perceptron. We will test our SVM on the same dataset and compare the results. Recall that to find the maximum-margin line has been reduced to solving an optimization problem with inequality constraints. To fit the iris data into this framework, we assign +1 to \"Iris-setosa\" and -1 to 'Iris-versicolor'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 3.1*** Generate a 2D NumPy array with 100 rows and 3 columns, named \"$A$\" (we need 100 rows as there are 100 samples in the iris dataset). Fill in the first column of $A$ by the 'sepal_length' values, the second column of $A$ by the 'sepal_width' values, and the third column of $A$ by $\\pm$ 1 according to the rule of assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will call the function \"cvxopt_solve_qp(P,q,G,h)\" again to solve the optimization problem. This time, we take \n",
    "$$\n",
    "w = \\left[\n",
    "\\begin{array}{c}\n",
    "w_1  \\\\\n",
    "w_2  \\\\\n",
    "w_0\n",
    "\\end{array}\n",
    "\\right], \\quad \n",
    "P = \\left[\n",
    "\\begin{array}{ccc}\n",
    "2 & 0 & 0 \\\\\n",
    "0 & 2 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{array}\n",
    "\\right], \\quad \n",
    "q = \\left[\n",
    "\\begin{array}{c}\n",
    "0  \\\\\n",
    "0  \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right], \\quad \n",
    "G = ???, \\quad\n",
    "h = \\left[\n",
    "\\begin{array}{c}\n",
    "-1  \\\\\n",
    "-1  \\\\\n",
    "\\vdots  \\\\\n",
    "-1\n",
    "\\end{array}\n",
    "\\right], \\quad \n",
    "$$\n",
    "Here $h$ is a vector with 100 components, all of which are -1's. The matrix $G$ is left for you to figure out. Recall that $G$ consists of the coefficients of $w_1,w_2,w_0$ in the inequalities. Since we have 100 samples in the iris dataset, you should expect to get 100 inequalities. This suggests that $G$ has 100 rows. Again, you *must* express all the inequalities in terms of only \"$\\leq$\" before collecting the coefficients of $w_1,w_2,w_0$ to from $G$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 3.2*** Construct $G$ and $h$ as matrices in \"cvxopt\", then call the function \"cvxopt_solve_qp(P,q,G,h)\" to compute the optimal values of $w_1,w_2,w_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 3.3*** Plot the iris samples as scattered points and the maximum-margin line you obtained from Question 3.2 in one figure. Compare this line with the one you found using Peceptron. Remember that the equation for the line is:\n",
    "\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + w_0 = 0\n",
    "$$\n",
    "\n",
    "which can be written in the $y = mx + b$ form like so:\n",
    "\n",
    "$$\n",
    "x_2 = -\\frac{w_1}{w_2} x_1 - \\frac{w_0}{w_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment wrap-up\n",
    "\n",
    "Please fill out the form that appears when you run the code below.  **You must completely fill this out in order to receive credit for the assignment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\n",
    "\"\"\"\n",
    "<iframe \n",
    "\tsrc=\"FIXME\" \n",
    "\twidth=\"800px\" \n",
    "\theight=\"600px\" \n",
    "\tframeborder=\"0\" \n",
    "\tmarginheight=\"0\" \n",
    "\tmarginwidth=\"0\">\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Congrats, you're done!\n",
    "\n",
    "Upload a copy of this notebook to the \"Homework #5\" dropbox on D2L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "## The details of the perceptron model (reproduced from the in-class assignment)\n",
    "\n",
    "The perceptron is a what is known as a basic binary classifier. It takes in a set of training data that is linerally separable and then computes a *set of weights* and a *bias term* to apply to input data as to properly classify it. Perceptrons only works for data that contains two classes and is linearly separable. For data that does not have these properties, the classifier cannot properly learn the weights and bias term.\n",
    "\n",
    "Since the perceptron is based on linearly separable data, we can think of the model as trying to learn the slope of a line, $y = m~x + b$. However in machine learning, we usually define $X$ to be an input *vector*, which is a $1$ by $N$ matrix, where $N$ is the number of measurements or \"features\" for a given sample. Then, we can create a similar matrix of weights, $W$, also of length N, and re-write our equation to be:\n",
    "\n",
    "$$ Y = W \\cdot X + B$$\n",
    "\n",
    "where $Y$ represents the resulting classification, consisting of either -1 or 1, depending on the output of $W \\cdot X$, the dot product of $W$ and $X$, and $B$ represents the bias term.  More explicitly, we can look at this in a matrix format:\n",
    "\n",
    "$$ Y = \\begin{bmatrix} w_1 & w_2 & \\dots & w_n \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} + B $$\n",
    "\n",
    "But, **how do we go about learning the weights of the model?**\n",
    "\n",
    "We learn the model weights by attempting to predict the class of our input data using an initial guess for the weights, and then update our weight values based on our prediction. We can define a \"step size\" for how much we change our weight between subsequent guess in the following way:\n",
    "\n",
    "`step_size = eta * (target class - predicted class)` \n",
    "\n",
    "Then, using our \"step size\", we then update all of our weights by multiplying our *step size times the corresponding feature values*:\n",
    "\n",
    "$$w_{1,new} = w_{1,old} + (\\mathrm{step~size} \\times x_{1})$$\n",
    "$$w_{2,new} = w_{2,old} + (\\mathrm{step~size} \\times x_{2})$$\n",
    "$$ \\vdots $$\n",
    "$$w_{N,new} = w_{N,old} + (\\mathrm{step~size} \\times x_{N})$$\n",
    "\n",
    "We also have to update our bias term, but just use the step size for this update: $B_{new} = B_{old} + \\mathrm{step~size}$.\n",
    "\n",
    "In this model, we use `eta` to represent our \"learning_rate\", which takes on a value between 0 and 1.\n",
    "\n",
    "The step size should always be a positive or negative decimal value depending on `eta`. For example, if we set the learning rate, `eta`, to be .1 and our target is -1 and we predict 1 then the we get the following equation.\n",
    "\n",
    "`step_size = .1 * (-1-1) = -.2`\n",
    "   \n",
    "Alternativey, if our target is 1 and we predict it as -1 then we will get the following.\n",
    "\n",
    "`step_size = .1 * (1 - -1) = .2`\n",
    "\n",
    "This process occurs iteratively. So, for a set number of iterations we calculate a step size and adjust the weights accordingly.\n",
    "\n",
    "**But, how do we handle the \"learning\" process when we have multiple samples?**\n",
    "\n",
    "In this case, we need to update the weights based on _all_ of the sample features. So our original equation above becomes:\n",
    "\n",
    "$$w_{1,new} = w_{1,old} + \\sum_{i=0}^{M} (\\mathrm{step~size} \\times x_{1,i})$$\n",
    "$$w_{2,new} = w_{2,old} + \\sum_{i=0}^{M} (\\mathrm{step~size} \\times x_{2,i})$$\n",
    "$$ \\vdots $$\n",
    "$$w_{N,new} = w_{N,old} + \\sum_{i=0}^{M} (\\mathrm{step~size} \\times x_{N,i})$$\n",
    "\n",
    "where $M$ is our total number of samples and we compute new weights for every features value.\n",
    "\n",
    "We also need to make sure we update the bias value in a similar way:\n",
    "\n",
    "$$B_{new} = B_{old} + \\sum_{i=0}^{M} (\\mathrm{step~size})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for how to set up the class from scratch (reproduced from the in-class assignment)\n",
    "\n",
    "> \n",
    "* Define the perceptron class with an `__init__` method\n",
    "    - The class should be initialized with the following attributes:\n",
    "        - user defined input value for `eta`, the learning rate for the perceptron.\n",
    "        - a number of interations to be used by the model, `n_iter`. This should also be an input parameter.\n",
    "        - an initial values for the bias (you can choose whether or not the user can set this values or if you want a standard default).\n",
    "* Create two methods for the perceptron clas, a `fit` method that does the learning and a `predict` method that outputs the predicted class\n",
    "    - The `fit` method should:\n",
    "        - define an array of weights the same length as the input vector. You can choose how to initialize the weight values.\n",
    "        - go through a set number of iterations (based on `n_iter`) where it makes predictions and updates the weights vector accordingly for each subsequent round of predictions.\n",
    "    - The `predict` method should:\n",
    "        - take in a feature vector and return the predicted class based on the current weights\n",
    "        - *hint*: The prediction is just a dot product of the weights and the features plus the bias term. The resulting ouput should be in the range (-1,1), depending on the value of the dot product. If the prediction is less than 0 it should return -1, otherwise it should return 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the decision boundary (reproduced from the in-class assignment)\n",
    "\n",
    "Finally, to better understand our classifier, it might help to plot what is known as the \"decision boundary\". The decision boundary is the line that seperates the classes in the classifier. The line is defined by the weights and the bias term that we calculated for our model.\n",
    "\n",
    "The slope of the decision boundary is defined as:\n",
    "\n",
    "$$ m = -\\frac{w_1}{w_2} $$\n",
    "\n",
    "And the $y$-intercept, $b$, is defined as:\n",
    "\n",
    "$$ b = -\\frac{B}{w_2} $$\n",
    "\n",
    "You should be able to generate a set of evenly spaced $x$-axis values and then used the equation for a line ($y = mx + b$) to compute the decision boundary for making a plot of the line. You should get something that looks like this:\n",
    "\n",
    "<img src=https://i.imgur.com/UPX8XDy.png>\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
